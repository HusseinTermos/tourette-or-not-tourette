{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4535,
     "status": "ok",
     "timestamp": 1759000847166,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "1E3BosDi10MK",
    "outputId": "2c3f23cf-d835-468e-a7d8-66dc58eb4392"
   },
   "outputs": [],
   "source": [
    "!pip install transformers librosa datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1979,
     "status": "ok",
     "timestamp": 1759000849149,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "9SwclcYm14Jv",
    "outputId": "7d44df72-30b5-4fe3-9de7-47a91e76342b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1759000849633,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "c150740f-186d-4c22-85b5-99a0917b999c",
    "outputId": "bd519eeb-6e92-423e-c23f-8843432fcc2e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "SR = 16000\n",
    "dataset_path = '/content/drive/MyDrive/tourrets_data'  # Assuming the data is in your Google Drive\n",
    "\n",
    "data = []\n",
    "for person_id in os.listdir(dataset_path):\n",
    "    person_dir = os.path.join(dataset_path, person_id)\n",
    "    if os.path.isdir(person_dir):\n",
    "        for label in ['0', '1']:\n",
    "            label_dir = os.path.join(person_dir, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                for filename in os.listdir(label_dir):\n",
    "                    if filename.endswith('.wav') or filename.endswith('.aac'):  # Updated to include both extensions\n",
    "                        file_path = os.path.join(label_dir, filename)\n",
    "                        data.append({'file_path': file_path, 'label': int(label)})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "executionInfo": {
     "elapsed": 6431,
     "status": "ok",
     "timestamp": 1759000856086,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "wNBm2kyAal09",
    "outputId": "124caa7e-3cce-47bd-920c-956ad6f3687a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "dataset_path = '/content/drive/MyDrive/tourrets_data'  # Assuming the data is in your Google Drive\n",
    "output_augmented_path = '/content/drive/MyDrive/tourrets_data_augmented' # Path to save augmented data\n",
    "\n",
    "# Define sliding window parameters (adjust as needed)\n",
    "window_size = SR * 2  # 2 seconds at 16kHz\n",
    "hop_size = int(SR * 0.5)     # 0.5 second hop\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_augmented_path, exist_ok=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # print('x')\n",
    "    file_path = row['file_path']\n",
    "    label = row['label']\n",
    "    person_id = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "    label_dir_name = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "        # Resample if necessary (assuming target SR is 16kHz)\n",
    "        if sr != SR:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, SR)\n",
    "            sr = SR # Update sample rate\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(0, keepdim=True)\n",
    "\n",
    "        # Apply sliding window\n",
    "        for start in range(0, waveform.shape[1] - window_size + 1, hop_size):\n",
    "            end = start + window_size\n",
    "            segment = waveform[:, start:end]\n",
    "\n",
    "            # Create directory structure for augmented data\n",
    "            augmented_person_dir = os.path.join(output_augmented_path, person_id)\n",
    "            augmented_label_dir = os.path.join(augmented_person_dir, label_dir_name)\n",
    "            os.makedirs(augmented_label_dir, exist_ok=True)\n",
    "\n",
    "            # Define save path for the segment\n",
    "            original_filename = os.path.basename(file_path)\n",
    "            segment_filename = f\"{os.path.splitext(original_filename)[0]}_segment_{start}_{end}.wav\"\n",
    "            segment_save_path = os.path.join(augmented_label_dir, segment_filename)\n",
    "\n",
    "            # Save the segment (using soundfile for broader format support if needed, or torchaudio.save)\n",
    "            sf.write(segment_save_path, segment.squeeze().numpy(), sr)\n",
    "\n",
    "\n",
    "            augmented_data.append({'file_path': segment_save_path, 'label': label})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file {file_path}: {e}\")\n",
    "        # Optionally, append the original file path if segmentation failed\n",
    "\n",
    "# Create a new DataFrame with augmented data\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Replace the original DataFrame with the augmented one\n",
    "df = df_augmented\n",
    "\n",
    "display(df.head())\n",
    "print(f\"Original dataset size: {len(data)}\")\n",
    "print(f\"Augmented dataset size: {len(df_augmented)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1759000856147,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "cmHQXc0V3bFz",
    "outputId": "5e6aba23-d964-4ad8-ee8d-dcc703c14533"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1759000856169,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "53fd1748"
   },
   "outputs": [],
   "source": [
    "# # !pip install -q --upgrade transformers torchaudio soundfile\n",
    "\n",
    "# import torch, torchaudio\n",
    "# from transformers import AutoFeatureExtractor, AutoModel  # or WavLMModel\n",
    "\n",
    "# feat_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/wavlm-base-plus\")  # or WavLMModel\n",
    "# model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1759000856179,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "jXMDt9UK4HXK"
   },
   "outputs": [],
   "source": [
    "# # Load audio (any format), convert to mono 16k\n",
    "# waveform, sr = torchaudio.load(\"drive/MyDrive/tourrets_data/01/0/00-00_00__to__00-03_00.aac\")     # [C, T]\n",
    "# if waveform.shape[0] > 1:\n",
    "#     waveform = waveform.mean(0, keepdim=True)\n",
    "# if sr != 16000:\n",
    "#     waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "\n",
    "# audio = waveform.squeeze().numpy()  # 1D float array in [-1, 1]\n",
    "\n",
    "# inputs = feat_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# print(outputs.last_hidden_state.shape)  # [batch, time_steps, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1759000856197,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "m61w2PRN-3XF"
   },
   "outputs": [],
   "source": [
    "# audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1759000856206,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "EMPp9J_u-5O0"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import WavLMModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# class TicClassifier(nn.Module):\n",
    "#     def __init__(self, backbone_name=\"microsoft/wavlm-base-plus\", dropout=0.2):\n",
    "#         super().__init__()\n",
    "#         self.feat_extractor = Wav2Vec2FeatureExtractor.from_pretrained(backbone_name)\n",
    "#         self.backbone = WavLMModel.from_pretrained(backbone_name)\n",
    "#         hidden = self.backbone.config.hidden_size  # 768 for base-plus\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(256, 1)  # binary logit\n",
    "#         )\n",
    "\n",
    "#     def forward(self, waveforms, sampling_rate=16000):\n",
    "#         \"\"\"\n",
    "#         waveforms: list[1D float arrays] or a padded tensor [B, T]\n",
    "#         returns: logits [B, 1]\n",
    "#         \"\"\"\n",
    "#         # Use the feature extractor to build inputs + attention_mask\n",
    "#         inputs = self.feat_extractor(\n",
    "#             waveforms, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True\n",
    "#         )\n",
    "#         input_values = inputs[\"input_values\"].to(self.backbone.device)      # [B, T]\n",
    "#         attention_mask = inputs[\"attention_mask\"].to(self.backbone.device)  # [B, T]\n",
    "\n",
    "#         outputs = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
    "#         hs = outputs.last_hidden_state  # [B, T', H]\n",
    "\n",
    "#         # Masked mean pooling over time (accounting for downsampling of T' vs attention_mask)\n",
    "#         # Align masks to hidden length if shapes differ\n",
    "#         if attention_mask.shape[1] != hs.shape[1]:\n",
    "#             # Downsample mask to hs length (nearest)\n",
    "#             am = nn.functional.interpolate(\n",
    "#                 attention_mask[:, None, :].float(), size=hs.shape[1], mode=\"nearest\"\n",
    "#             ).squeeze(1)\n",
    "#         else:\n",
    "#             am = attention_mask\n",
    "\n",
    "#         am = am.unsqueeze(-1)  # [B, T', 1]\n",
    "#         masked = hs * am\n",
    "#         denom = am.sum(dim=1).clamp_min(1e-6)  # [B, 1]\n",
    "#         pooled = masked.sum(dim=1) / denom     # [B, H]\n",
    "\n",
    "#         logits = self.classifier(pooled)       # [B, 1]\n",
    "#         return logits.squeeze(1)               # [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1759000856213,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "RRkPGTSM_2Sq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "219a47e0"
   },
   "source": [
    "# Task\n",
    "Fine-tune the WavLM model for Tourette's tic detection using the provided dataset located in the \"tourrets_data\" directory, which contains audio files in both .wav and .aac formats organized by person ID and labeled with 1 for positive samples and 0 for negative samples. Train the `TicClassifier` model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0431a195"
   },
   "source": [
    "## Prepare the dataset for training\n",
    "\n",
    "### Subtask:\n",
    "Create a PyTorch Dataset and DataLoader to handle loading, preprocessing, and batching of your audio data and labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f494a265"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a PyTorch Dataset and DataLoader to handle the audio data and labels. This involves defining a custom Dataset class to load and preprocess the audio files and their corresponding labels, and then creating a DataLoader instance for batching.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1759000856230,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "8340e372"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.dataframe.iloc[idx]['file_path']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(0, keepdim=True)\n",
    "            if sr != SR:\n",
    "                waveform = torchaudio.functional.resample(waveform, sr, SR)\n",
    "\n",
    "            audio = waveform.squeeze().numpy()\n",
    "            # print(\"len audio: \", len(audio), self.dataframe.iloc[idx]['file_path'])\n",
    "            return {\"audio\": audio, \"label\": label}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None # Return None for problematic samples\n",
    "\n",
    "\n",
    "dataset = AudioDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=lambda x: {k: [dic[k] for dic in x if dic is not None] for k in x[0]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7806b92d"
   },
   "source": [
    "## Define training parameters\n",
    "\n",
    "### Subtask:\n",
    "Set up the optimizer, loss function, learning rate, number of epochs, and other relevant training parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a49a85f"
   },
   "source": [
    "**Reasoning**:\n",
    "I need to set up the optimizer, loss function, learning rate, and other relevant training parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1759000856237,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "bc644ed0"
   },
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# # Define training parameters\n",
    "# learning_rate = 1e-2\n",
    "# num_epochs = 10\n",
    "# weight_decay = 1e-5\n",
    "\n",
    "# # Choose optimizer\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# # Choose loss function (Binary Cross-Entropy with Logits)\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e1e001e"
   },
   "source": [
    "## Implement the training loop\n",
    "\n",
    "### Subtask:\n",
    "Write the code for the training process, including forward pass, loss calculation, backpropagation, and weight updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1759000856935,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "458e2c56",
    "outputId": "a6008f16-b6ce-4c4f-dde9-723d1cc7e9d9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Extract person_id from file_path and add it as a new column to the DataFrame\n",
    "df['person_id'] = df['file_path'].apply(lambda x: os.path.basename(os.path.dirname(os.path.dirname(x))))\n",
    "\n",
    "# Get unique person IDs\n",
    "unique_person_ids = df['person_id'].unique()\n",
    "\n",
    "# Split unique person IDs into training, validation, and testing sets\n",
    "train_ids, test_ids = train_test_split(unique_person_ids, test_size=0.2, random_state=42)\n",
    "# train_ids, val_ids = train_test_split(train_ids, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2 of original\n",
    "\n",
    "# Create training, validation, and testing DataFrames\n",
    "train_df = df[df['person_id'].isin(train_ids)]\n",
    "# val_df = df[df['person_id'].isin(val_ids)]\n",
    "test_df = df[df['person_id'].isin(test_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "# print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")\n",
    "\n",
    "display(train_df.head())\n",
    "# display(val_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "359044b2"
   },
   "source": [
    "**Reasoning**:\n",
    "The error message \"TypeError: list indices must be integers or slices, not tuple\" indicates that the input to the model's feature extractor is a list of tensors, but it expects a single tensor. This is because the `collate_fn` in the DataLoader is returning a list of tensors for the 'audio' key. I need to modify the `collate_fn` to pad the audio data and return a single tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1759000856965,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "664b8ea7",
    "outputId": "abf8fcba-e73a-49b1-ea10-87c95021fc9c"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import numpy as np\n",
    "\n",
    "# Assuming AudioDataset class is defined\n",
    "# Assuming train_df, val_df, and test_df DataFrames are defined\n",
    "# Assuming feat_extractor is defined\n",
    "\n",
    "# Add truncation to collate_fn\n",
    "def collate_fn_with_truncation(batch, max_len=SR * 2): # Example max_len (10 seconds at 16kHz)\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    # Separate audio and labels\n",
    "    audios = [item['audio'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    # Truncate and pad audio data\n",
    "    padded_audios = rnn_utils.pad_sequence(\n",
    "        [torch.tensor(audio[:max_len]) for audio in audios],  # Truncate here\n",
    "        batch_first=True\n",
    "    )\n",
    "\n",
    "    return {\"audio\": padded_audios, \"label\": torch.tensor(labels)}\n",
    "\n",
    "\n",
    "# Create AudioDataset instances for each split\n",
    "train_dataset = AudioDataset(train_df)\n",
    "# val_dataset = AudioDataset(val_df)\n",
    "test_dataset = AudioDataset(test_df)\n",
    "\n",
    "# Define batch size (can be adjusted based on GPU memory)\n",
    "batch_size = 8 # You might need to adjust this based on your GPU memory\n",
    "\n",
    "# Create DataLoader instances for each split\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_with_truncation)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_with_truncation)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_with_truncation)\n",
    "\n",
    "print(\"DataLoaders created for training, validation, and testing.\")\n",
    "# You can optionally print the number of batches in each dataloader to verify\n",
    "# print(f\"Number of batches in training dataloader: {len(train_dataloader)}\")\n",
    "# print(f\"Number of batches in validation dataloader: {len(len(val_dataloader))}\")\n",
    "# print(f\"Number of batches in testing dataloader: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1759000856978,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "HuiaA4ANkefe",
    "outputId": "b3b3117f-d9ef-4d25-e6e9-04f81855eaf8"
   },
   "outputs": [],
   "source": [
    "len(train_dataset),len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1759001310701,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "85971ddf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_binary_classifier(model, loader, threshold=0.5, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a binary audio classifier.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model (already trained).\n",
    "        loader: DataLoader yielding batches shaped like either\n",
    "                {\"audio\": padded_tensor[B,T], \"label\": tensor/list} OR\n",
    "                {\"waves\": list(np1d/tensor1d), \"labels\": tensor}.\n",
    "        device: torch.device\n",
    "        threshold: probability threshold applied to sigmoid(logits).\n",
    "        verbose: print metrics if True.\n",
    "\n",
    "    Returns:\n",
    "        dict with accuracy, precision, recall, f1, confusion matrix, TN/FP/FN/TP,\n",
    "        and FP/FN ratios relative to positives/negatives.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs  = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            # Get waves and labels in a loader-agnostic way\n",
    "            if \"waves\" in batch:\n",
    "                waves = []\n",
    "                for w in batch[\"waves\"]:\n",
    "                    if isinstance(w, torch.Tensor):\n",
    "                        waves.append(w.detach().cpu().float().numpy())\n",
    "                    else:\n",
    "                        waves.append(np.asarray(w, dtype=np.float32))\n",
    "                labels = batch.get(\"labels\", batch.get(\"label\"))\n",
    "            else:\n",
    "                # {\"audio\": padded_tensor, \"label\": ...}\n",
    "                waves = batch[\"audio\"]             # TicClassifier can accept this directly\n",
    "                labels = batch.get(\"label\", batch.get(\"labels\"))\n",
    "\n",
    "            if not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            # Forward pass (model handles devices internally for the classifier head)\n",
    "            logits = model(waves)                  # [B]\n",
    "            probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.asarray(all_labels, dtype=int)\n",
    "    y_pred = (np.asarray(all_probs) >= float(threshold)).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm   = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Extract counts if 2x2\n",
    "    tn = fp = fn = tp = 0\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        total_pos = tp + fn\n",
    "        total_neg = tn + fp\n",
    "        fp_ratio = fp / total_pos if total_pos > 0 else 0.0\n",
    "        fn_ratio = fn / total_neg if total_neg > 0 else 0.0\n",
    "    else:\n",
    "        fp_ratio = fn_ratio = 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Test Accuracy:  {acc:.4f}\")\n",
    "        print(f\"Test Precision: {prec:.4f}\")\n",
    "        print(f\"Test Recall:    {rec:.4f}\")\n",
    "        print(f\"Test F1 Score:  {f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        if cm.shape == (2, 2):\n",
    "            print(f\"True Negatives (TN): {tn}\")\n",
    "            print(f\"False Positives (FP): {fp}\")\n",
    "            print(f\"False Negatives (FN): {fn}\")\n",
    "            print(f\"True Positives (TP): {tp}\")\n",
    "            print(f\"Ratio of False Positives to Total Positives: {fp_ratio:.4f}\")\n",
    "            print(f\"Ratio of False Negatives to Total Negatives: {fn_ratio:.4f}\")\n",
    "        else:\n",
    "            print(\"Confusion matrix shape is not 2x2, cannot compute FP/FN ratios.\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1,\n",
    "        \"cm\": cm, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp,\n",
    "        \"fp_to_total_positives_ratio\": fp_ratio,\n",
    "        \"fn_to_total_negatives_ratio\": fn_ratio,\n",
    "        \"y_true\": y_true, \"y_pred\": y_pred, \"y_prob\": np.asarray(all_probs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 443041,
     "status": "error",
     "timestamp": 1759001300021,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "lZMSKrHcC73-",
    "outputId": "fb91c290-2530-45d6-ead8-c9d9dab29195"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import WavLMModel, Wav2Vec2FeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TicClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    YAMNet backbone (TF-Hub) -> mean-pooled 1024-D embeddings -> PyTorch classifier head.\n",
    "    Expects mono 16 kHz float32 audio in [-1, 1].\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.2, yamnet_handle=\"https://tfhub.dev/google/yamnet/1\"):\n",
    "        super().__init__()\n",
    "        self.yamnet = hub.load(yamnet_handle)  # frozen TF model\n",
    "        self.sample_rate = SR\n",
    "        hidden = 1024  # YAMNet embedding dimension\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)  # binary logit\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _yamnet_embed(self, wav_np: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        wav_np: 1-D numpy float32 at 16 kHz.\n",
    "        Returns: embeddings [num_frames, 1024] (0.48 s hop, ~0.96 s window).\n",
    "        \"\"\"\n",
    "        x = np.asarray(wav_np, dtype=np.float32)\n",
    "        # YAMNet returns (scores, embeddings, spectrogram)\n",
    "        _, embeddings, _ = self.yamnet(x)\n",
    "        return embeddings.numpy()  # [frames, 1024]\n",
    "\n",
    "    def forward(self, waveforms, max_len_s: float = None):\n",
    "        \"\"\"\n",
    "        waveforms: either padded tensor [B, T] or list of 1-D arrays/tensors.\n",
    "        max_len_s: optional cap per-sample seconds (crop random during train, center at eval).\n",
    "        returns: logits [B]\n",
    "        \"\"\"\n",
    "        # --- normalize batch input to list of 1-D float32 numpy arrays on CPU ---\n",
    "        if isinstance(waveforms, torch.Tensor):\n",
    "            batch = [w.detach().cpu().float().numpy() for w in waveforms]\n",
    "        else:\n",
    "            batch = []\n",
    "            for w in waveforms:\n",
    "                if isinstance(w, torch.Tensor):\n",
    "                    batch.append(w.detach().cpu().float().numpy())\n",
    "                else:\n",
    "                    batch.append(np.asarray(w, dtype=np.float32))\n",
    "\n",
    "        # --- optional cropping to bound compute ---\n",
    "        if max_len_s is not None:\n",
    "            L = int(self.sample_rate * max_len_s)\n",
    "            for i, w in enumerate(batch):\n",
    "                if w.shape[0] > L:\n",
    "                    start = np.random.randint(0, w.shape[0] - L + 1) if self.training else (w.shape[0] - L) // 2\n",
    "                    batch[i] = w[start:start + L]\n",
    "\n",
    "        # --- trim trailing padding zeros (if your collate padded with zeros) ---\n",
    "        for i, w in enumerate(batch):\n",
    "            nz = np.flatnonzero(np.abs(w) > 1e-7)\n",
    "            if nz.size > 0:\n",
    "                batch[i] = w[: nz[-1] + 1]\n",
    "\n",
    "        # --- run YAMNet per sample, mean-pool embeddings ---\n",
    "        pooled = []\n",
    "        for w in batch:\n",
    "            try:\n",
    "                emb = self._yamnet_embed(w)  # [frames, 1024]\n",
    "                if emb.shape[0] == 0:\n",
    "                    pooled.append(np.zeros(1024, dtype=np.float32))\n",
    "                else:\n",
    "                    pooled.append(emb.mean(axis=0).astype(np.float32))\n",
    "            except Exception:\n",
    "                # on any TF/audio error, fall back to zeros to keep training robust\n",
    "                pooled.append(np.zeros(1024, dtype=np.float32))\n",
    "\n",
    "        feats = torch.from_numpy(np.stack(pooled, axis=0)).to(next(self.classifier.parameters()).device)  # [B,1024]\n",
    "        logits = self.classifier(feats)  # [B,1]\n",
    "        return logits.squeeze(1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    # Separate audio and labels\n",
    "    audios = [item['audio'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    # Pad audio data\n",
    "    padded_audios = torch.nn.utils.rnn.pad_sequence([torch.tensor(audio) for audio in audios], batch_first=True)\n",
    "\n",
    "    return {\"audio\": padded_audios, \"label\": torch.tensor(labels)}\n",
    "\n",
    "\n",
    "dataloader = train_dataloader\n",
    "\n",
    "model = TicClassifier() # Re-instantiate the model with the corrected forward pass\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define training parameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Choose optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Choose loss function (Binary Cross-Entropy with Logits)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "fp = 0\n",
    "fn = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # cop += 1\n",
    "        # print(batch)\n",
    "        # exit()\n",
    "        if batch is None:\n",
    "            continue # Skip empty batches\n",
    "\n",
    "        broke = False\n",
    "        for x in batch[\"audio\"]:\n",
    "            if len(x)  >= 1e5:\n",
    "              broke = True\n",
    "              break\n",
    "        if broke: continue\n",
    "        audio_data = batch['audio'].to(device)\n",
    "        labels = batch['label'].float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(audio_data)\n",
    "        # print(logits, \"ll\")\n",
    "        for i in range(len(logits)):\n",
    "          if logits[i] > 0.5 and batch['label'][i] == 0:\n",
    "              fp += 1\n",
    "          if logits[i] <= 0.5 and batch['label'][i] == 1:\n",
    "              fn += 1\n",
    "\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # print(total_loss)\n",
    "        # if cop > 10: break\n",
    "        # Explicitly delete tensors and clear cache more frequently\n",
    "        del audio_data, labels, logits, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Optional: Clear cache after every few iterations\n",
    "        if (i + 1) % 10 == 0: # Clear cache every 10 iterations\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n",
    "\n",
    "print(\"fp\", fp/num_epochs)\n",
    "print(\"fn\", fn/num_epochs)\n",
    "# Clear cache and collect garbage after training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 457682,
     "status": "aborted",
     "timestamp": 1759001300013,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "xuuRQ16sDv0l"
   },
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 457693,
     "status": "aborted",
     "timestamp": 1759001300026,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "3YFrRHDLEg_7"
   },
   "outputs": [],
   "source": [
    "total_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1759001315968,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "HZfR1v3hK5qf",
    "outputId": "903fba24-b7d5-4170-9ab4-91397e6f69a6"
   },
   "outputs": [],
   "source": [
    "evaluate_binary_classifier(model, test_dataloader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1759002048999,
     "user": {
      "displayName": "hussein termos",
      "userId": "01172407318232329970"
     },
     "user_tz": -180
    },
    "id": "KsGQH6TsoaKg",
    "outputId": "4c828a1e-cfc8-4b13-8187-309d80576f93"
   },
   "outputs": [],
   "source": [
    "# After training:\n",
    "SAVE_DIR = \"drive/MyDrive/artifacts\"\n",
    "import os, json, torch\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Save ONLY the PyTorch classifier head\n",
    "torch.save(model.classifier.state_dict(), f\"{SAVE_DIR}/classifier_head.pt\")\n",
    "\n",
    "# 2) Save a small config so the server knows how to rebuild the model\n",
    "config = {\n",
    "    \"yamnet_handle\": \"https://tfhub.dev/google/yamnet/1\",\n",
    "    \"sr\": 16000,\n",
    "    \"dropout\": 0.2,\n",
    "    \"max_len_s\": 2.0,\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "with open(f\"{SAVE_DIR}/model_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", os.listdir(SAVE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7wWi8iasTeU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPfT5VjqRYaB+lSZNqxPa3D",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
